{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from snake import SnakeGame\n",
    "from agent import A2CAgent\n",
    "import mlflow\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MLFLOW_URI = os.getenv(\"MLFLOW_URI\")\n",
    "if not MLFLOW_URI:\n",
    "    raise Exception(\"MLFLOW_URI is not set\")\n",
    "\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "def train_agents(episodes):\n",
    "    mlflow.set_experiment(\"Snake Game A2C Training\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        env = SnakeGame(10, 10, max_steps=20000)\n",
    "        agent = A2CAgent(device=device)\n",
    "        batch_size = 64\n",
    "        rewards = []\n",
    "        \n",
    "        # Log parameters\n",
    "        log_param(\"episodes\", episodes)\n",
    "        log_param(\"batch_size\", batch_size)\n",
    "        log_param(\"gamma\", agent.gamma)\n",
    "        log_param(\"learning_rate\", agent.lr)\n",
    "        log_param(\"memory_size\", agent.maxlen)\n",
    "        \n",
    "        # Initialize metric accumulators\n",
    "        metric_buffer = {\n",
    "            \"episode_reward\": [],\n",
    "            \"episode_length\": [],\n",
    "            \"total_loss\": [],\n",
    "            \"actor_loss\": [],\n",
    "            \"critic_loss\": [],\n",
    "            \"entropy\": []\n",
    "        }\n",
    "        \n",
    "        for e in range(episodes):\n",
    "            state = env.reset()\n",
    "            acc_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                acc_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "            \n",
    "            losses = agent.replay(batch_size)\n",
    "            rewards.append(acc_reward)\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            metric_buffer[\"episode_reward\"].append(acc_reward)\n",
    "            metric_buffer[\"episode_length\"].append(steps)\n",
    "            if losses is not None:\n",
    "                total_loss, actor_loss, critic_loss, entropy = losses\n",
    "                metric_buffer[\"total_loss\"].append(total_loss)\n",
    "                metric_buffer[\"actor_loss\"].append(actor_loss)\n",
    "                metric_buffer[\"critic_loss\"].append(critic_loss)\n",
    "                metric_buffer[\"entropy\"].append(entropy)\n",
    "            \n",
    "            # Log metrics every 100 episodes\n",
    "            if e % 100 == 0 and e > 0:\n",
    "                for metric, values in metric_buffer.items():\n",
    "                    if values:\n",
    "                        log_metric(metric, sum(values) / len(values), step=e)\n",
    "                metric_buffer = {k: [] for k in metric_buffer}\n",
    "            \n",
    "            if e % 100_000 == 0 and e > 0:\n",
    "                print(f\"episode={e}\")\n",
    "                # Log model every 100k episodes\n",
    "                mlflow.pytorch.log_model(agent.model, f\"model_episode_{e}\")\n",
    "        \n",
    "        # Log final model and rewards\n",
    "        mlflow.pytorch.log_model(agent.model, \"final_model\")\n",
    "        agent.save(\"./agent_10x10.state\")\n",
    "        log_artifacts(\"agent_10x10.state\")\n",
    "        \n",
    "        # Save rewards to a file and log as artifact\n",
    "        torch.save(rewards, \"./rewards_10x10.state\")\n",
    "        log_artifacts(\"rewards_10x10.state\")\n",
    "    \n",
    "    return agent, rewards\n",
    "\n",
    "# Run the training\n",
    "agent, rewards = train_agents(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved to agent_10x10.state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maurits/miniconda3/envs/develop/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 16.8762 s\n",
      "File: /tmp/ipykernel_2588604/82477549.py\n",
      "Function: train_agents at line 10\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    10                                           def train_agents(episodes):\n",
      "    11         1    6232367.0    6e+06      0.0      mlflow.set_experiment(\"Snake Game A2C Training\")\n",
      "    12                                               \n",
      "    13         2   14530720.0    7e+06      0.1      with mlflow.start_run():\n",
      "    14         1     138733.0 138733.0      0.0          env = SnakeGame(10, 10, max_steps=20000)\n",
      "    15         1    1566002.0    2e+06      0.0          agent = A2CAgent(device=device)\n",
      "    16         1        150.0    150.0      0.0          batch_size = 64\n",
      "    17         1        150.0    150.0      0.0          rewards = []\n",
      "    18                                                   \n",
      "    19                                                   # Log parameters\n",
      "    20         1    3759891.0    4e+06      0.0          log_param(\"episodes\", episodes)\n",
      "    21         1    3639052.0    4e+06      0.0          log_param(\"batch_size\", batch_size)\n",
      "    22         1    3657597.0    4e+06      0.0          log_param(\"gamma\", agent.gamma)\n",
      "    23         1    3664571.0    4e+06      0.0          log_param(\"learning_rate\", agent.lr)\n",
      "    24         1    4059437.0    4e+06      0.0          log_param(\"memory_size\", agent.maxlen)\n",
      "    25                                                   \n",
      "    26                                                   # Initialize metric accumulators\n",
      "    27         1        441.0    441.0      0.0          metric_buffer = {\n",
      "    28         1        171.0    171.0      0.0              \"episode_reward\": [],\n",
      "    29         1         61.0     61.0      0.0              \"episode_length\": [],\n",
      "    30         1         70.0     70.0      0.0              \"total_loss\": [],\n",
      "    31         1         70.0     70.0      0.0              \"actor_loss\": [],\n",
      "    32         1         70.0     70.0      0.0              \"critic_loss\": [],\n",
      "    33         1         70.0     70.0      0.0              \"entropy\": []\n",
      "    34                                                   }\n",
      "    35                                                   \n",
      "    36      1001     232010.0    231.8      0.0          for e in range(episodes):\n",
      "    37      1000   66183391.0  66183.4      0.4              state = env.reset()\n",
      "    38      1000     155514.0    155.5      0.0              acc_reward = 0\n",
      "    39      1000      88436.0     88.4      0.0              done = False\n",
      "    40      1000      95011.0     95.0      0.0              steps = 0\n",
      "    41                                                       \n",
      "    42     18971    2870035.0    151.3      0.0              while not done:\n",
      "    43     17971 9043709636.0 503239.1     53.6                  action = agent.act(state)\n",
      "    44     17971  583931116.0  32493.0      3.5                  next_state, reward, done = env.step(action)\n",
      "    45     17971   19843693.0   1104.2      0.1                  agent.remember(state, action, reward, next_state, done)\n",
      "    46     17971    2541733.0    141.4      0.0                  acc_reward += reward\n",
      "    47     17971    1773036.0     98.7      0.0                  state = next_state\n",
      "    48     17971    2274069.0    126.5      0.0                  steps += 1\n",
      "    49                                                       \n",
      "    50      1000 2529925403.0    3e+06     15.0              losses = agent.replay(batch_size)\n",
      "    51      1000     598309.0    598.3      0.0              rewards.append(acc_reward)\n",
      "    52                                                       \n",
      "    53                                                       # Accumulate metrics\n",
      "    54      1000     408841.0    408.8      0.0              metric_buffer[\"episode_reward\"].append(acc_reward)\n",
      "    55      1000     283770.0    283.8      0.0              metric_buffer[\"episode_length\"].append(steps)\n",
      "    56      1000     184995.0    185.0      0.0              if losses is not None:\n",
      "    57       996     429794.0    431.5      0.0                  total_loss, actor_loss, critic_loss, entropy = losses\n",
      "    58       996     280339.0    281.5      0.0                  metric_buffer[\"total_loss\"].append(total_loss)\n",
      "    59       996     253156.0    254.2      0.0                  metric_buffer[\"actor_loss\"].append(actor_loss)\n",
      "    60       996     249453.0    250.5      0.0                  metric_buffer[\"critic_loss\"].append(critic_loss)\n",
      "    61       996     259685.0    260.7      0.0                  metric_buffer[\"entropy\"].append(entropy)\n",
      "    62                                                       \n",
      "    63                                                       # Log metrics every 100 episodes\n",
      "    64      1000     416171.0    416.2      0.0              if e % 100 == 0 and e > 0:\n",
      "    65        63      42295.0    671.3      0.0                  for metric, values in metric_buffer.items():\n",
      "    66        54       9453.0    175.1      0.0                      if values:\n",
      "    67        54  223457622.0    4e+06      1.3                          log_metric(metric, sum(values) / len(values), step=e)\n",
      "    68         9      58288.0   6476.4      0.0                  metric_buffer = {k: [] for k in metric_buffer}\n",
      "    69                                                       \n",
      "    70      1000     370939.0    370.9      0.0              if e % 100_000 == 0 and e > 0:\n",
      "    71                                                           print(f\"episode={e}\")\n",
      "    72                                                           # Log model every 100k episodes\n",
      "    73                                                           mlflow.pytorch.log_model(agent.model, f\"model_episode_{e}\")\n",
      "    74                                                   \n",
      "    75                                                   # Log final model and rewards\n",
      "    76         1 4283182070.0    4e+09     25.4          mlflow.pytorch.log_model(agent.model, \"final_model\")\n",
      "    77         1   69506883.0    7e+07      0.4          agent.save(\"./agent_10x10.state\")\n",
      "    78         1     465640.0 465640.0      0.0          log_artifacts(\"agent_10x10.state\")\n",
      "    79                                                   \n",
      "    80                                                   # Save rewards to a file and log as artifact\n",
      "    81         1     574326.0 574326.0      0.0          torch.save(rewards, \"./rewards_10x10.state\")\n",
      "    82         1     269720.0 269720.0      0.0          log_artifacts(\"rewards_10x10.state\")\n",
      "    83                                               \n",
      "    84         1        171.0    171.0      0.0      return agent, rewards"
     ]
    }
   ],
   "source": [
    "\n",
    "# Profile the train_agents function\n",
    "%lprun -f train_agents train_agents(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-07-11 04:48:42 2625213:2625213 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-07-11 04:48:42 2625213:2625213 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-07-11 04:48:42 2625213:2625213 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      aten::multinomial        16.09%      11.173ms        48.45%      33.649ms     336.490us       7.441ms         9.40%      33.907ms     339.070us           100  \n",
      "                                               aten::to         2.00%       1.390ms         9.89%       6.866ms      21.191us       1.655ms         2.09%       7.716ms      23.815us           324  \n",
      "                                              aten::sum         5.88%       4.083ms        10.85%       7.532ms      37.287us       3.850ms         4.86%       7.566ms      37.455us           202  \n",
      "                                               aten::mm         5.28%       3.670ms         7.41%       5.145ms      12.021us       6.510ms         8.23%       6.510ms      15.210us           428  \n",
      "                                         aten::_to_copy         4.24%       2.942ms         7.89%       5.476ms      25.235us       2.280ms         2.88%       6.061ms      27.931us           217  \n",
      "                                            aten::addmm         5.05%       3.505ms         7.71%       5.351ms      26.230us       4.994ms         6.31%       5.973ms      29.279us           204  \n",
      "                                    aten::empty_strided         4.87%       3.385ms         4.87%       3.385ms       3.795us       5.816ms         7.35%       5.816ms       6.520us           892  \n",
      "                                             aten::item         3.01%       2.092ms         7.28%       5.053ms      15.405us       1.930ms         2.44%       5.640ms      17.195us           328  \n",
      "                          triton_poi_fused_leaky_relu_0         4.65%       3.232ms         5.85%       4.063ms       9.958us       5.456ms         6.89%       5.456ms      13.373us           408  \n",
      "                              aten::_local_scalar_dense         1.05%     728.000us         4.26%       2.958ms       9.018us       3.710ms         4.69%       3.710ms      11.311us           328  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 69.444ms\n",
      "Self CUDA time total: 79.139ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.profiler as profiler \n",
    "\n",
    "def profile_train_step(agent, env, pre_simulate_games=300):\n",
    "    # Pre-simulate games to fill the replay buffer\n",
    "    for _ in range(pre_simulate_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        agent.replay(256)\n",
    "\n",
    "\n",
    "    # Now start profiling\n",
    "    state = env.reset()\n",
    "    with profiler.profile(use_cuda=True) as prof:\n",
    "        for _ in range(100):  # Profile 100 steps\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "        losses = agent.replay(256)\n",
    "    \n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "\n",
    "env = SnakeGame(width=10, height=10, max_steps=20000)\n",
    "agent = A2CAgent(device=device)\n",
    "profile_train_step(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
