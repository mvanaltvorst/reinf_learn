{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from snake import SnakeGame\n",
    "from agent import A2CAgent\n",
    "import mlflow\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MLFLOW_URI = os.getenv(\"MLFLOW_URI\")\n",
    "if not MLFLOW_URI:\n",
    "    raise Exception(\"MLFLOW_URI is not set\")\n",
    "\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maurits/micromamba/envs/develop/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent, rewards\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m agent, rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m, in \u001b[0;36mtrain_agents\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m total_loss, actor_loss, critic_loss \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mreplay(batch_size)\n\u001b[1;32m     42\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(acc_reward)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "import os\n",
    "\n",
    "def train_agents(episodes):\n",
    "    # Set up MLflow\n",
    "    mlflow.set_experiment(\"Snake Game A2C Training\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        env = SnakeGame(10, 10, max_steps=20000)\n",
    "        agent = A2CAgent()\n",
    "        batch_size = 64\n",
    "        rewards = []\n",
    "        \n",
    "        # Log parameters\n",
    "        log_param(\"episodes\", episodes)\n",
    "        log_param(\"batch_size\", batch_size)\n",
    "        log_param(\"gamma\", agent.gamma)\n",
    "        log_param(\"learning_rate\", agent.lr)\n",
    "        log_param(\"memory_size\", agent.maxlen)\n",
    "        \n",
    "        for e in range(episodes):\n",
    "            state = env.reset()\n",
    "            acc_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                acc_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            losses = agent.replay(batch_size)\n",
    "            rewards.append(acc_reward)\n",
    "            \n",
    "            # Log metrics\n",
    "            log_metric(\"episode_reward\", acc_reward, step=e)\n",
    "            log_metric(\"episode_length\", steps, step=e)\n",
    "            if losses is not None:\n",
    "                total_loss, actor_loss, critic_loss = losses\n",
    "                log_metric(\"total_loss\", total_loss, step=e)\n",
    "                log_metric(\"actor_loss\", actor_loss, step=e)\n",
    "                log_metric(\"critic_loss\", critic_loss, step=e)\n",
    "            \n",
    "            if e % 5000 == 0:\n",
    "                print(f\"episode={e}\")\n",
    "                # Log model every 5000 episodes\n",
    "                mlflow.pytorch.log_model(agent.model, f\"model_episode_{e}\")\n",
    "        \n",
    "        # Log final model and rewards\n",
    "        mlflow.pytorch.log_model(agent.model, \"final_model\")\n",
    "        agent.save(\"./agent_10x10.state\")\n",
    "        log_artifacts(\"agent_10x10.state\")\n",
    "        \n",
    "        # Save rewards to a file and log as artifact\n",
    "        torch.save(rewards, \"./rewards_10x10.state\")\n",
    "        log_artifacts(\"rewards_10x10.state\")\n",
    "    \n",
    "    return agent, rewards\n",
    "\n",
    "# Run the training\n",
    "agent, rewards = train_agents(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
